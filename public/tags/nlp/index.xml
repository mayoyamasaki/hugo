<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on Mayo&#39;s Blog</title>
    <link>https://mayoyamasaki.github.io/tags/nlp/</link>
    <description>Recent content in Nlp on Mayo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 18 Nov 2016 00:04:32 +0900</lastBuildDate>
    <atom:link href="https://mayoyamasaki.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>K-Means Clustering of Word2Vec on Python</title>
      <link>https://mayoyamasaki.github.io/content/post/kmeans%20clustering%20of%20word2vec%20on%20python/</link>
      <pubDate>Fri, 18 Nov 2016 00:04:32 +0900</pubDate>
      
      <guid>https://mayoyamasaki.github.io/content/post/kmeans%20clustering%20of%20word2vec%20on%20python/</guid>
      <description>

&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;scikit-learnのK-Means実装を使って、学習済みWord2Vecのクラスタリングを行った。&lt;/li&gt;
&lt;li&gt;それなりに上手く、クラスタリングできていそうだった。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;ACL2014で、EmbeddingsのクラスタをNER(Named Entity Recognition)に使用している論文がある&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;br /&gt;
線形モデルには、低次元連続値の素性(特徴量)より、高次元離散値の素性が良いらしい。&lt;br /&gt;
この記事では、Word2Vecで学習した単語ベクトル表現(連続値)を使って、K-Meansによるクラスタリング(離散値)を行ってみる。&lt;/p&gt;

&lt;h2 id=&#34;設定&#34;&gt;設定&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://code.google.com/archive/p/word2vec/&#34;&gt;Google Code word2vec&lt;/a&gt;にて公開されている、GoogleNews-vectors-negative300.bin.gzを入力に用いた。&lt;/li&gt;
&lt;li&gt;PyPIで公開されている&lt;a href=&#34;https://github.com/danielfrg/word2vec&#34;&gt;word2vec&lt;/a&gt;がTypeErrorで上手くモデルファイルをロードできなかったので、gensimの実装を使った。&lt;/li&gt;
&lt;li&gt;K-Meansには、データ量が大きかったので、scikit-learnの&lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans.fit&#34;&gt;MiniBatchKMeans&lt;/a&gt;を用いた&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;学習済みのWord2Vecのモデルファイルはいろいろ公開されていて、3Topというstartupのまとめがよかった&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;br /&gt;
gensimのWord2Vec実装のAPIドキュメント&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;は、あんまり丁寧に書かれていないので、結構つらかった。&lt;/p&gt;

&lt;h2 id=&#34;実装&#34;&gt;実装&lt;/h2&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;argparse&lt;/span&gt;
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;functools&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; lru_cache
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;logging&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; getLogger

&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;numpy&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;np&lt;/span&gt;
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;gensim.models.word2vec&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; Word2Vec
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;sklearn.cluster&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; MiniBatchKMeans
&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color: #0000FF; font-weight: bold&#34;&gt;sklearn.externals&lt;/span&gt; &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;import&lt;/span&gt; joblib


logger &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; getLogger(__name__)


&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color: #0000FF&#34;&gt;make_dataset&lt;/span&gt;(model):
    &lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Make dataset from pre-trained Word2Vec model.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    Paramters&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    ---------&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    model: gensim.models.word2vec.Word2Vec&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;        pre-traind Word2Vec model as gensim object.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    Returns&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    -------&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    numpy.ndarray((vocabrary size, vector size))&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;        Sikitlearn&amp;#39;s X format.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    V &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;index2word
    X &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;zeros((&lt;span style=&#34;color: #008000&#34;&gt;len&lt;/span&gt;(V), model&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;vector_size))

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;for&lt;/span&gt; index, word &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color: #008000&#34;&gt;enumerate&lt;/span&gt;(V):
        X[index, :] &lt;span style=&#34;color: #666666&#34;&gt;+=&lt;/span&gt; model[word]
    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;return&lt;/span&gt; X


&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color: #0000FF&#34;&gt;train&lt;/span&gt;(X, K):
    &lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Learn K-Means Clustering with MiniBatchKMeans.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    Paramters&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    ---------&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    X: numpy.ndarray((sample size, feature size))&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;        training dataset.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    K: int&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;        number of clusters to use MiniBatchKMeans.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    Returens&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    --------&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    sklearn.cluster.MiniBatchKMeans&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;        trained model.&lt;/span&gt;
&lt;span style=&#34;color: #BA2121; font-style: italic&#34;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    logger&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;info(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;start to fiting KMeans with {} classs.&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;format(K))
    classifier &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; MiniBatchKMeans(n_clusters&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;K, random_state&lt;span style=&#34;color: #666666&#34;&gt;=0&lt;/span&gt;)
    classifier&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;fit(X)
    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;return&lt;/span&gt; classifier


&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color: #0000FF&#34;&gt;main&lt;/span&gt;():
    parser &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; argparse&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;ArgumentParser(
        description&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;Python Word2Vec Cluster&amp;#39;&lt;/span&gt;)

    parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;model&amp;#39;&lt;/span&gt;,
                        action&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;store&amp;#39;&lt;/span&gt;,
                        help&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;Name of word2vec binary modelfile.&amp;#39;&lt;/span&gt;)

    parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;-o&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;--out&amp;#39;&lt;/span&gt;,
                        action&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;store&amp;#39;&lt;/span&gt;,
                        default&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;model.pkl&amp;#39;&lt;/span&gt;,
                        help&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;Set output filename.&amp;#39;&lt;/span&gt;)

    parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;-k&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;--K&amp;#39;&lt;/span&gt;,
                        action&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;store&amp;#39;&lt;/span&gt;,
                        &lt;span style=&#34;color: #008000&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000&#34;&gt;int&lt;/span&gt;,
                        default&lt;span style=&#34;color: #666666&#34;&gt;=500&lt;/span&gt;,
                        help&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;Num of classes on KMeans.&amp;#39;&lt;/span&gt;)

    parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;-p&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;--pre-trained-model&amp;#39;&lt;/span&gt;,
                        action&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;store&amp;#39;&lt;/span&gt;,
                        default&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000&#34;&gt;None&lt;/span&gt;,
                        help&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;Use pre-trained KMeans Model.&amp;#39;&lt;/span&gt;)

    parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;-w&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;--words-to-pred&amp;#39;&lt;/span&gt;,
                        action&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;store&amp;#39;&lt;/span&gt;,
                        nargs&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt;,
                        &lt;span style=&#34;color: #008000&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000&#34;&gt;str&lt;/span&gt;,
                        default&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000&#34;&gt;None&lt;/span&gt;,
                        help&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;List of word to predict.&amp;#39;&lt;/span&gt;)

    args &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; parser&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;parse_args()

    model &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; Word2Vec&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;load_word2vec_format(args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;model, binary&lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #008000&#34;&gt;True&lt;/span&gt;)

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;not&lt;/span&gt; args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;pre_trained_model:
        X &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; make_dataset(model)
        classifier &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; train(X, args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;K)
        joblib&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;dump(classifier, args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;out)
    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;else&lt;/span&gt;:
        classifier &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; joblib&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;load(args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;pre_trained_model)

    &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;if&lt;/span&gt; args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;words_to_pred:

        X &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; [model[word] &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;for&lt;/span&gt; word &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;in&lt;/span&gt; args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;words_to_pred &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;if&lt;/span&gt; word &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;in&lt;/span&gt; model]
        classes &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; classifier&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;predict(X)

        result &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; []
        i &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;0&lt;/span&gt;
        &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;for&lt;/span&gt; word &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;in&lt;/span&gt; args&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;words_to_pred:
            &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;if&lt;/span&gt; word &lt;span style=&#34;color: #AA22FF; font-weight: bold&#34;&gt;in&lt;/span&gt; model:
                result&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color: #008000&#34;&gt;str&lt;/span&gt;(classes[i]))
                i &lt;span style=&#34;color: #666666&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;else&lt;/span&gt;:
                result&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color: #008000&#34;&gt;str&lt;/span&gt;(&lt;span style=&#34;color: #666666&#34;&gt;-1&lt;/span&gt;))
        &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;.&lt;/span&gt;join(result))


&lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color: #666666&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;:
    main()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;実装は、Githubに公開している通りだが、Word2Vecの辞書をNumpyのndarrayに変換するやり方が、最適ではない気がする。まあ一度だけしか実行しないので、今回は気にしない&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;h3 id=&#34;学習&#34;&gt;学習&lt;/h3&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;$ python3 w2vcluster/w2vcluster.py GoogleNews-vectors-negative300.bin -k &lt;span style=&#34;color: #666666&#34;&gt;500&lt;/span&gt; -o model1000.pkl
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;予測&#34;&gt;予測&lt;/h3&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;python3 w2vcluster/w2vcluster.py GoogleNews-vectors-negative300.bin -p model500.pkl -w apple Apple banana Google
&lt;span style=&#34;color: #666666&#34;&gt;176&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;118&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;176&lt;/span&gt; 118
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;argparseの可変長リストは初めて使った。見ての通り、appleとbananaはクラスタID 176に、AppleとGoogleはクラスタID 118に分類されており、上手くいっているような気がする。
モデルファイルは、joblibのpickleファイルなので、もちろんPythonからも使うことができる。やり方、READMEに一応書いている。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;MiniBatchKMeansを使うと、大規模データでもMacbook Proで問題なく計算できた(数分程度で終わるが、通常のKMeansだと終わる気配がなかった&amp;hellip;)。&lt;br /&gt;
離散化したWord2Vecを何らのモデルでつかってみたい。ソフトクラスタリングも試してみたい。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;http://aclweb.org/anthology/D/D14/D14-1012.pdf&#34;&gt;Jiang Guo et al. Revisiting Embedding Features for Simple Semi-supervised Learning&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;&lt;a href=&#34;http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py&#34;&gt;Comparison of the K-Means and MiniBatchKMeans clustering algorithms&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;&lt;a href=&#34;https://github.com/3Top/word2vec-api&#34;&gt;word2vec-api&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;&lt;a href=&#34;https://radimrehurek.com/gensim/models/word2vec.html&#34;&gt;models.word2vec – Deep learning with word2vec&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;&lt;a href=&#34;https://github.com/mayoyamasaki/py-kmeans-word2vec&#34;&gt;Python K-Means Cluster of Word2Vec&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>HTML to TEXT for NLP</title>
      <link>https://mayoyamasaki.github.io/content/post/html-to-text-for-nlp/</link>
      <pubDate>Fri, 04 Nov 2016 21:52:39 +0900</pubDate>
      
      <guid>https://mayoyamasaki.github.io/content/post/html-to-text-for-nlp/</guid>
      <description>

&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;

&lt;p&gt;「HTMLからテキストを取り出す」というタスクは、一見簡単の様で難しい。&lt;br /&gt;
特に、後続するテキスト処理(文分割や形態素解析、構文解析)を上手くしようとすると難しい。&lt;br /&gt;
ここでは、英語文書に対して、ナイーブなルールで、テキスト抽出をそこそこ上手くやる方法を紹介する。&lt;/p&gt;

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;この記事では、Github上で公開している実装の簡単な紹介をする&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;p&gt;使い方は、以下のようにコマンドラインツールとして使ったり、&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;python3 html2text.py https://example.com &amp;gt; example.com.txt
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Pythonを通して利用できる。&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;$ python3
&amp;gt;&amp;gt;&amp;gt; import html2text
&amp;gt;&amp;gt;&amp;gt; &lt;span style=&#34;color: #19177C&#34;&gt;html&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; html2text.url2html&lt;span style=&#34;color: #666666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color: #BA2121&#34;&gt;&amp;#39;https://example.com&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #666666&#34;&gt;)&lt;/span&gt;
&amp;gt;&amp;gt;&amp;gt; &lt;span style=&#34;color: #19177C&#34;&gt;bodyhtml&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; html2text.html2body&lt;span style=&#34;color: #666666&#34;&gt;(&lt;/span&gt;html&lt;span style=&#34;color: #666666&#34;&gt;)&lt;/span&gt;
&amp;gt;&amp;gt;&amp;gt; &lt;span style=&#34;color: #19177C&#34;&gt;text&lt;/span&gt; &lt;span style=&#34;color: #666666&#34;&gt;=&lt;/span&gt; html2text.html2text&lt;span style=&#34;color: #666666&#34;&gt;(&lt;/span&gt;bodyhtml&lt;span style=&#34;color: #666666&#34;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&#34;実装&#34;&gt;実装&lt;/h2&gt;

&lt;h3 id=&#34;スペースの統一&#34;&gt;スペースの統一&lt;/h3&gt;

&lt;p&gt;タブ文字、&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%8E%E3%83%BC%E3%83%96%E3%83%AC%E3%83%BC%E3%82%AF%E3%82%B9%E3%83%9A%E3%83%BC%E3%82%B9&#34;&gt;NBSP&lt;/a&gt;&lt;code&gt;\xa0&lt;/code&gt;等を半角スペースに置換する。
NBSPは、よく構文解析などが例外を吐くので注意。&lt;/p&gt;

&lt;h3 id=&#34;ブロックタグ削除の影響&#34;&gt;ブロックタグ削除の影響&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;foo.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;bar&lt;/code&gt;の様なケースでは、単に削除すると、&lt;code&gt;foo.bar&lt;/code&gt;となり、TokenizeやSentence segmentationで失敗する原因になる。また&lt;code&gt;foo.&amp;lt;/div&amp;gt;bar&lt;/code&gt;の様なケースもあるので、単にブロックタグの連接だけでなく、ブロックタグの直後に空白や改行無しに、文字が連接するかをチェックする必要がある。&lt;br /&gt;
ここでは、前者に対しては改行、後者に対してはスペースを挿入することで対処している。&lt;/p&gt;

&lt;h3 id=&#34;タグ削除による影響&#34;&gt;タグ削除による影響&lt;/h3&gt;

&lt;p&gt;ブロックタグに限らず、CSSでblockやinline-block等をしていしているインラインタグでも、同種の影響がでる。もっと言えば、HTML&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;https://github.com/mayoyamasaki/html2text&#34;&gt;https://github.com/mayoyamasaki/html2text&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>